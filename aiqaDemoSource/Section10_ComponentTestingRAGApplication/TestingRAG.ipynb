{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing RAG Applications ðŸ“‘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RAG Application\n",
    "This application reads data about Model Context Protocol (MCP) server from internet, stores in vector stores, chunks the data with embedding and useful to answer the question about MCP while inferenced.\n",
    "\n",
    "<img src=\"./img/RAG.png\" width=\"500\" height=\"400\" style=\"display: block; margin: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU langchain-chroma\n",
    "\n",
    "#!pip install -U DeepEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n",
       "</pre>\n"
      ],
      "text/plain": [
       "ðŸŽ‰ðŸ¥³ Congratulations! You've successfully logged in! ðŸ™Œ \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import deepeval\n",
    "\n",
    "deepeval.login_with_confident_api_key(\"o6wy2TTe0igTiXs6zs6/JnR+wfzws96MGYfsqGOzntA=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ™Œ Congratulations! You're now using a local Ollama model for all evals that \n",
      "require an LLM.\n"
     ]
    }
   ],
   "source": [
    "!deepeval set-ollama deepseek-r1:8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from typing import List\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.document import Document\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import AnswerRelevancyMetric, ContextualRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.tracing import (\n",
    "    observe,\n",
    "    update_current_span,\n",
    "    RetrieverAttributes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(type='llm', model='qwen2.5:latest')\n",
    "def local_llms():\n",
    "    # return ChatOllama(\n",
    "    #     base_url=\"http://localhost:11434\",\n",
    "    #     model = \"qwen2.5:latest\",\n",
    "    #     temperature=0.5,\n",
    "    #     max_tokens = 250\n",
    "    # )\n",
    "    return ChatOpenAI(model=\"gpt-4.1-2025-04-14\", max_completion_tokens=300)\n",
    "    \n",
    "llm = local_llms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">[</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">Confident AI Trace Log</span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f; font-weight: bold\">]</span>  <span style=\"color: #800000; text-decoration-color: #800000\">Error posting trace </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">(</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">0</span><span style=\"color: #800000; text-decoration-color: #800000\"> traces remaining in queue, </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">1</span><span style=\"color: #800000; text-decoration-color: #800000\"> in flight</span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">)</span>: Object of type SecretStr \n",
       "is not JSON serializable\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;2m[\u001b[0m\u001b[2mConfident AI Trace Log\u001b[0m\u001b[1;2m]\u001b[0m  \u001b[31mError posting trace \u001b[0m\u001b[1;31m(\u001b[0m\u001b[1;31m0\u001b[0m\u001b[31m traces remaining in queue, \u001b[0m\u001b[1;31m1\u001b[0m\u001b[31m in flight\u001b[0m\u001b[1;31m)\u001b[0m: Object of type SecretStr \n",
       "is not JSON serializable\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load data from Web\n",
    "loader = WebBaseLoader(\"https://www.descope.com/learn/post/mcp\")\n",
    "data = loader.load()\n",
    "\n",
    "# Split text into documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "splits = text_splitter.split_documents(data)\n",
    "\n",
    "# Add text to vector db\n",
    "# embedding = OllamaEmbeddings(model=\"llama3.2:latest\")\n",
    "embedding = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "vectordb = Chroma.from_documents(documents=splits, embedding=embedding)\n",
    "\n",
    "# Create a retriever\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "def format_docs(docs: List[Document]) -> str:\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "\n",
    "    {context}\n",
    "    \n",
    "    Give a summary not the full detail\n",
    "\n",
    "    Question: {question}\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "@observe(metrics=[AnswerRelevancyMetric()])\n",
    "def retrieve_and_format(question):\n",
    "    docs = retriever.invoke(question)\n",
    "    response = format_docs(docs)\n",
    "    \n",
    "    update_current_span(\n",
    "        test_case=LLMTestCase(input=question, actual_output=response)\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "chain = {\"context\": retrieve_and_format, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@observe(type=\"retriever\", embedder=\"text-embedding-3-large\")\n",
    "def retrive_documents(question):\n",
    "    retrived_context = retrieve_and_format(question)\n",
    "    \n",
    "    update_current_span(\n",
    "        attributes= RetrieverAttributes(\n",
    "            embedding_input=question,\n",
    "            retrieval_context= [retrived_context]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    return retrived_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the LLM Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@observe(type=\"custom\", name=\"RAG Application\", metrics=[ContextualRelevancyMetric()])\n",
    "def rag_application(question):\n",
    "    actual_response = chain.invoke(question)\n",
    "    retrived_context = retrive_documents(question)\n",
    "    \n",
    "    update_current_span(\n",
    "        test_case=LLMTestCase(input=question, actual_output=actual_response, retrieval_context=[retrived_context])\n",
    "    )\n",
    "    \n",
    "    return actual_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of RAG Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating goldens: |          |  0% (0/1) [Time Taken: 00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ending trace: [BaseSpan(uuid='adda6f3e-26b9-4ff5-b9c4-31ca57886ee0', status=<TraceSpanStatus.SUCCESS: 'SUCCESS'>, children=[BaseSpan(uuid='ae59fd9a-85de-49a8-8eb0-96b1ed5a1806', status=<TraceSpanStatus.SUCCESS: 'SUCCESS'>, children=[BaseSpan(uuid='f2919f80-7f23-45f1-a8e9-e5a2ab556597', status=<TraceSpanStatus.SUCCESS: 'SUCCESS'>, children=[], trace_uuid='525eb692-c1d8-44cf-828d-a3c1ca310b28', parent_uuid='ae59fd9a-85de-49a8-8eb0-96b1ed5a1806', start_time=304779.015910291, end_time=304779.883650208, name='retrieve_and_format', metadata=None, input={'question': 'What is MCP'}, output=\"development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour GitHub) to\\n\\nplatforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as examples\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightCatch up on all the updates from AI Launch Week! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of\\n\\nhostâ€™s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary transport\", error=None, llm_test_case=LLMTestCase(input='What is MCP', actual_output=\"development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour GitHub) to\\n\\nplatforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as examples\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightCatch up on all the updates from AI Launch Week! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of\\n\\nhostâ€™s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary transport\", expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, token_cost=None, completion_time=None, name=None), metrics=[<deepeval.metrics.answer_relevancy.answer_relevancy.AnswerRelevancyMetric object at 0x124284fb0>]), RetrieverSpan(uuid='923662e6-d7fe-4e23-88c4-5412f96eb852', status=<TraceSpanStatus.SUCCESS: 'SUCCESS'>, children=[BaseSpan(uuid='9b4df421-f1cd-4e91-b6e3-995afb290df8', status=<TraceSpanStatus.SUCCESS: 'SUCCESS'>, children=[], trace_uuid='525eb692-c1d8-44cf-828d-a3c1ca310b28', parent_uuid='923662e6-d7fe-4e23-88c4-5412f96eb852', start_time=304782.460428666, end_time=304783.457735041, name='retrieve_and_format', metadata=None, input={'question': 'What is MCP'}, output=\"development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour GitHub) to\\n\\nplatforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as examples\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightCatch up on all the updates from AI Launch Week! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of\\n\\nhostâ€™s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary transport\", error=None, llm_test_case=LLMTestCase(input='What is MCP', actual_output=\"development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour GitHub) to\\n\\nplatforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as examples\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightCatch up on all the updates from AI Launch Week! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of\\n\\nhostâ€™s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary transport\", expected_output=None, context=None, retrieval_context=None, additional_metadata=None, comments=None, tools_called=None, expected_tools=None, token_cost=None, completion_time=None, name=None), metrics=[<deepeval.metrics.answer_relevancy.answer_relevancy.AnswerRelevancyMetric object at 0x124284fb0>])], trace_uuid='525eb692-c1d8-44cf-828d-a3c1ca310b28', parent_uuid='ae59fd9a-85de-49a8-8eb0-96b1ed5a1806', start_time=304782.460363916, end_time=304783.457835166, name='retrive_documents', metadata=None, input='What is MCP', output=[\"development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour GitHub) to\\n\\nplatforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as examples\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightCatch up on all the updates from AI Launch Week! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of\\n\\nhostâ€™s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary transport\"], error=None, llm_test_case=None, metrics=None, embedder='text-embedding-3-large', attributes=RetrieverAttributes(embedding_input='What is MCP', retrieval_context=[\"development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour GitHub) to\\n\\nplatforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as examples\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightCatch up on all the updates from AI Launch Week! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of\\n\\nhostâ€™s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary transport\"], top_k=None, chunk_size=None))], trace_uuid='525eb692-c1d8-44cf-828d-a3c1ca310b28', parent_uuid='adda6f3e-26b9-4ff5-b9c4-31ca57886ee0', start_time=304779.008333, end_time=304783.457894833, name='RAG Application', metadata=None, input={'question': 'What is MCP'}, output='MCP, or Model Context Protocol, is a standard that enables AI applications to access and use external tools and data through a unified protocol. It defines how AI apps (clients) interact with various servers that provide specific functions or integrations (e.g., accessing GitHub or databases). MCP aims to reduce development overhead, improve interoperability, and allow innovations to be shared across the AI ecosystem rather than remain siloed in individual projects.', error=None, llm_test_case=LLMTestCase(input='What is MCP', actual_output='MCP, or Model Context Protocol, is a standard that enables AI applications to access and use external tools and data through a unified protocol. It defines how AI apps (clients) interact with various servers that provide specific functions or integrations (e.g., accessing GitHub or databases). MCP aims to reduce development overhead, improve interoperability, and allow innovations to be shared across the AI ecosystem rather than remain siloed in individual projects.', expected_output=None, context=None, retrieval_context=[\"development overhead and enables a more interoperable ecosystem where innovation benefits the entire communityâ€”rather than remaining siloed.As MCP continues to progress as a standard, several new developments have appeared on the horizon:Official MCP registry: A maintainer-sanctioned registry for MCP servers is being planned, which will simplify discovery and integration of available tools. This centralized repository will make it easier for anyone (not just those willing to scour GitHub) to\\n\\nplatforms like Superinterface, which helps developers build in-app AI assistants with MCP functionality.Examples of MCP serversThe MCP ecosystem comprises a diverse range of servers including reference servers (created by the protocol maintainers as implementation examples), official integrations (maintained by companies for their platforms), and community servers (developed by independent contributors).Reference serversReference servers demonstrate core MCP functionality and serve as examples\\n\\nWhat Is the Model Context Protocol (MCP) and How It WorksSkip to main contentArrow RightCatch up on all the updates from AI Launch Week! Let's go >Log InUser CircleProductUse CasesDevelopersCustomersResourcesCompanyPricingSign upArrow RightBook a demoArrow RightIdentipediaArrow LeftWhat Is the Model Context Protocol (MCP) and How It Works April 7, 2025Copy linkShare on:Share on LinkedInShare on XShare on BluskyTable of ContentsLLM isolation & the NxM problemOpen table of contentsTable of\\n\\nhostâ€™s requirements and the Model Context Protocol. Clients are built into host applications, like the MCP client inside Claude Desktop.MCP server: Adds context and capabilities, exposing specific functions to AI apps through MCP. Each standalone server typically focuses on a specific integration point, like GitHub for repository access or a PostgreSQL for database operations. Transport layer: The communication mechanism between clients and servers. MCP supports two primary transport\"], additional_metadata=None, comments=None, tools_called=None, expected_tools=None, token_cost=None, completion_time=None, name=None), metrics=[<deepeval.metrics.contextual_relevancy.contextual_relevancy.ContextualRelevancyMetric object at 0x17fccb2c0>])], trace_uuid='525eb692-c1d8-44cf-828d-a3c1ca310b28', parent_uuid=None, start_time=304779.008157958, end_time=304783.457929291, name='Test Wrapper', metadata=None, input={}, output=None, error=None, llm_test_case=None, metrics=None)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Evaluating goldens: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|100% (1/1) [Time Taken: 00:46, 46.88s/it]\n",
      "     âš¡ Invoking traceable callback: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|100% (1/1) [Time Taken: 00:46, 46.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What is MCP\n",
      "  - actual output: None\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: None\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #05f58d; text-decoration-color: #05f58d\">âœ“</span> Tests finished ðŸŽ‰! View results on \n",
       "<a href=\"https://app.confident-ai.com/project/cmb8sq46q07rf1tfo1k6r68x4/evaluation/test-runs/cmbbhq7gc00gcepdzs0lnpcim/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://app.confident-ai.com/project/cmb8sq46q07rf1tfo1k6r68x4/evaluation/test-runs/cmbbhq7gc00gcepdzs0lnpcim/compa</span></a>\n",
       "<a href=\"https://app.confident-ai.com/project/cmb8sq46q07rf1tfo1k6r68x4/evaluation/test-runs/cmbbhq7gc00gcepdzs0lnpcim/compare-test-results\" target=\"_blank\"><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">re-test-results</span></a><span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;5;245;141mâœ“\u001b[0m Tests finished ðŸŽ‰! View results on \n",
       "\u001b]8;id=738904;https://app.confident-ai.com/project/cmb8sq46q07rf1tfo1k6r68x4/evaluation/test-runs/cmbbhq7gc00gcepdzs0lnpcim/compare-test-results\u001b\\\u001b[4;94mhttps://app.confident-ai.com/project/cmb8sq46q07rf1tfo1k6r68x4/evaluation/test-runs/cmbbhq7gc00gcepdzs0lnpcim/compa\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b]8;id=738904;https://app.confident-ai.com/project/cmb8sq46q07rf1tfo1k6r68x4/evaluation/test-runs/cmbbhq7gc00gcepdzs0lnpcim/compare-test-results\u001b\\\u001b[4;94mre-test-results\u001b[0m\u001b]8;;\u001b\\\u001b[4;94m.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "EvaluationResult(test_results=[TestResult(name='test_case_1', success=True, metrics_data=[], conversational=False, multimodal=False, input='What is MCP', actual_output=None, expected_output=None, context=None, retrieval_context=None, additional_metadata=None)], confident_link='https://app.confident-ai.com/project/cmb8sq46q07rf1tfo1k6r68x4/evaluation/test-runs/cmbbhq7gc00gcepdzs0lnpcim/compare-test-results')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval.dataset import Golden\n",
    "from deepeval import evaluate\n",
    "\n",
    "goldens = Golden(input=\"What is MCP\")\n",
    "evaluate(goldens=[goldens], observed_callback=rag_application)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
